{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop Unused Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_1_path = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "output_file_1 = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "\n",
    "df1 = pd.read_csv(file_1_path, encoding='ISO-8859-1', low_memory=False)[['comment_text', 'toxic']]\n",
    "df1.to_csv(output_file_1, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename Column Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: .\\Ori_Dataset\\ToxicData.csv\n"
     ]
    }
   ],
   "source": [
    "file_1_path = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "\n",
    "output_file_1 = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "\n",
    "def dataset (file_path,output_csv_path):\n",
    "    df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "    df.rename(columns={'toxic': 'label', 'comment_text': 'text'}, inplace=True)\n",
    "    df.to_csv(output_csv_path, index=False, encoding='ISO-8859-1')\n",
    "    print(f\"Processed and saved: {output_csv_path}\")\n",
    "dataset (file_1_path,output_file_1)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column before drop:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(\"\\nMissing values per column before drop:\")\n",
    "print(data.isnull().sum())\n",
    "# print(f\"\\nTotal rows before drop: {len(data)}\\n\")\n",
    "\n",
    "# data = data.dropna()\n",
    "\n",
    "# print(f\"Total rows after drop: {len(data)}\")\n",
    "\n",
    "# output_file = r\".\\Pre_Dataset\\1_Combine.csv\"\n",
    "# data.to_csv(output_file, index=False, encoding='ISO-8859-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    explanation\\nwhy the edits made under my usern...\n",
      "1    d'aww! he matches this background colour i'm s...\n",
      "2    hey man, i'm really not trying to edit war. it...\n",
      "3    \"\\nmore\\ni can't make any real suggestions on ...\n",
      "4    you, sir, are my hero. any chance you remember...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Ori_Dataset\\ToxicData.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "print(df['text'].head())\n",
    "\n",
    "file = r\".\\Pre_Dataset\\1_Lowercasing.csv\"\n",
    "df.to_csv(file, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column before drop:\n",
      "text     0\n",
      "label    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Dataset\\1_Lowercasing.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "print(\"\\nMissing values per column before drop:\")\n",
    "print(data.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 43\n",
      "Number of duplicated rows (after Drop) : 0\n"
     ]
    }
   ],
   "source": [
    "file_path = r\".\\Pre_Dataset\\1_Lowercasing.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")\n",
    "\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\1_Lowercasing.csv\"\n",
    "data.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows (after Drop) : {duplicate_rows.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\".\\Pre_Dataset\\1_Lowercasing.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Remove URLs\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\b(?:http|https|www)\\S+|\\b\\S+\\.(com|net|org|io|gov|edu|cn)\\b', '', str(x)))\n",
    "\n",
    "# Remove HTML\n",
    "html_tags_pattern = r'<.*?>'\n",
    "# sub(pattern,replace,text)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(html_tags_pattern, ' ', str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\S+html\\b', ' ', str(x)))\n",
    "\n",
    "# Remove emojis\n",
    "def remove_emojis(text):\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r':\\w+:', ' ', text)\n",
    "    return text\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "\n",
    "# Remove ASCII characters & delete unused punctuation\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"_\", \" \"))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r\"[^A-Za-z0-9\\s']\", ' ', str(x)))\n",
    "df['text'] = df['text'].apply(lambda x: x.replace(\"'\", \"\"))\n",
    "\n",
    "# Remove excessive whitespace\n",
    "# .strip() removes any leading or trailing whitespace from the text\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "\n",
    "# Remove only number rows\n",
    "only_numbers_df = df[df['text'].astype(str).str.strip().str.isdigit()]\n",
    "# print(only_numbers_df)\n",
    "df = df[~df['text'].astype(str).str.strip().str.isdigit()] #turn all data into string (astype(str), remove space (strip()), check is whole string is digit)\n",
    "\n",
    "# Removing elongation (example: goodddddddddd)\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'(.)\\1{2,}', r'\\1\\1', x))\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\2_remove.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace Abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to .\\Pre_Dataset\\3_Replace_Abbreviations.csv\n"
     ]
    }
   ],
   "source": [
    "# Replace Abbreviations\n",
    "file_path = r\".\\Pre_Dataset\\2_remove.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Abbreviations List base on unknow word diagram\n",
    "abbreviations = {\n",
    "    \"werent\":\"were not\",\"arent\": \"are not\",\n",
    "    \"isnt\": \"is not\",\n",
    "    \"cant\": \"can not\",\n",
    "    \"hasnt\": \"has not\",\n",
    "    \"hadnt\": \"had not\",\n",
    "    \"shes\": \"she is\",\"hes\": \"he is\",\n",
    "    \"youre\": \"you are\", \n",
    "    \"youll\": \"you will\",\n",
    "    \"youve\": \"you have\",\n",
    "    \"weve\": \"we have\",\n",
    "    \"yall\":\"you all\",\n",
    "    \"theyre\": \"they are\", \n",
    "    \"theyve\": \"they have\",\n",
    "    \"doesnt\": \"does not\", \n",
    "    \"dont\":\"do not\",\n",
    "    \"didnt\": \"did not\",\n",
    "    \"wont\": \"will not\",\n",
    "    \"wouldnt\": \"would not\",\n",
    "    \"shouldnt\": \"should not\",\n",
    "    \"couldnt\": \"could not\",\n",
    "    \"im\": \"i am\",\n",
    "    \"iam\": \"i am\",\n",
    "    \"ive\": \"i have\",\n",
    "    \"id\": \"i would\",\n",
    "    \"cuz\":\"because\", \"bcuz\":\"because\",\"becuz\":\"because\",\n",
    "    \"fucksex\": \"fuck sex\",\n",
    "    \"yourselfgo\": \"yourself go\",\n",
    "    \"bitchesfuck\": \"bitches fuck\",\n",
    "    \"youfuck\": \"you fuck\",\n",
    "    \"diff\": \"different\",\n",
    "    \"backgroundcolor\": \"background color\",\n",
    "}\n",
    "def replace_abbreviations(text):\n",
    "    text = str(text)\n",
    "    for abbr, full_form in abbreviations.items():\n",
    "        text = re.sub(r'\\b' + re.escape(abbr) + r'\\b', full_form, text)\n",
    "    return str(text)\n",
    "df['text'] = df['text'].apply(replace_abbreviations)\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\3_Replace_Abbreviations.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"Data has been saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "file_path = r\".\\Pre_Dataset\\4_Removed_Repeats_word.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "filteredTokens = []\n",
    "for token in df['text']:\n",
    "    token = str(token)\n",
    "    wordtokens = nltk.tokenize.word_tokenize(token)\n",
    "    filteredTokens.append(wordtokens)\n",
    "df['text']=filteredTokens\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\5_Tokenization.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Stop Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Word\n",
    "file_path = r\".\\Pre_Dataset\\5_Tokenization.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Convert strings back to lists, stored as strings in the CSV file \n",
    "df['text'] = df['text'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) #(Yadav, 2023)\n",
    "\n",
    "stopTokens = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "def removeStopWord(words):\n",
    "    return [word for word in words if word.lower() not in stopTokens]\n",
    "df['text'] = df['text'].apply(removeStopWord)\n",
    "\n",
    "output_file_path =r\".\\Pre_Dataset\\6_Remove_StopWord.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\".\\Pre_Dataset\\6_Remove_StopWord.csv\"\n",
    "df = pd.read_csv(file_path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') #used for Part-of-Speech (POS) tagging\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_pos_tagging(word):\n",
    "    #[0][1]:('running', 'VBG')\n",
    "    #[0][1][0]:('V')\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV} #need this for wordnet cuz wordnet only have 4 postag\n",
    "    return tag_dict.get(tag, wordnet.NOUN)  # Default to noun if no match\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_pos_tagging(word)) for word in text] \n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\7_Lemmatization.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159528 entries, 0 to 159527\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159485 non-null  object\n",
      " 1   label   159528 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check NULL Column\n",
    "file_path = r\".\\Pre_Dataset\\7_Lemmatization.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 1309\n"
     ]
    }
   ],
   "source": [
    "# check duplicated\n",
    "file_path = r\".\\Pre_Dataset\\8_Repeated.csv\"\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
    "\n",
    "duplicate_rows = data[data.duplicated()]\n",
    "print(f\"Number of duplicated rows: {duplicate_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\".\\Pre_Dataset\\7_Lemmatization.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "def remove_excessive_repetition(text, phrase_len=2, repeat=5):\n",
    "    words = text.split()\n",
    "    cleaned = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(words):\n",
    "        phrase = ' '.join(words[i:i + phrase_len])\n",
    "        count = 1\n",
    "        j = i + phrase_len\n",
    "\n",
    "        while j + phrase_len <= len(words) and ' '.join(words[j:j + phrase_len]) == phrase:\n",
    "            count += 1\n",
    "            j += phrase_len\n",
    "\n",
    "        if count >= repeat:\n",
    "            cleaned.extend(phrase.split())  \n",
    "            i = j\n",
    "        else:\n",
    "            cleaned.append(words[i])\n",
    "            i += 1\n",
    "\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: remove_excessive_repetition(x, phrase_len=2, repeat=5))\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\8_Repeated.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop na and duplicated\n",
    "file_path = r\".\\Pre_Dataset\\8_Repeated.csv\"\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "def remove_repeated_words(text):\n",
    "    if isinstance(text, str):\n",
    "        return re.sub(r'\\b(\\w+)( \\1\\b)+', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(remove_repeated_words)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "output_file_path = r\".\\Pre_Dataset\\Final.csv\"\n",
    "df.to_csv(output_file_path, index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
